# db-migration-tool

This tool can be used to migrate the data in a given collection from a source database in one Fauna Region Group to another target database, which can be in an entrirely different Region Group.
It uses Fauna's temporality feature to replicate all the write events that occurred after a given timestamp in the source collection, into the same collection of a target database. This tool can be use in combination with [Fauna's Datbase Copy](https://docs.fauna.com/fauna/current/administration/backups#create-a-database-from-a-snapshot) feature to achieve a database migration. By itself, this tool does not copy over or migrate indexes or keys.

The general procedure for migrating from on Region Group to another is to create a backup snapshot of the source database, create a new database from that snapshot on the desired Region Group. Once the database copy is complete, use this tool to synchronize the writes that have occurred on the sources database since the snapshot was taked. More guidance regard migration is included at the end of this README.

## Pre-requisites

- Take a snapshot of the source database. Please refer to the [Backups documentation](https://docs.fauna.com/fauna/current/administration/backups) for more information.
- Enable history_days on all collections that need to be migrated. This is done by altering the each collection's [document](https://docs.fauna.com/fauna/current/reference/schema_entities/collection/document_definition#fields).
 - To avoid gaps in data, be sute to set history_days to a period greater than the cadence you intend run this tool. For instance, if you only intend to sync on a weekly basis, set hitory_days to a value greater than 7
- This tool will automatically create a new index in the source collection, that is necessary to properly sync data from the source collection. These autogenerated indexes vary on how long it takes for thier build process to complete, which depends on the size of the colleciton. The index needs to be complete and active before this tool can sync the the collection data. index of the following shape will be created:
  `{ name: "<index-name>", source: Collection("<collection-name>"), terms: [], values: [ { field: "ts" }, { field: "ref" } ] }`

## Usage
```
Options:
  -v, --version              output the version number
  -s, --source <string>      access secret for the source DB
  -t, --target <string>      access secret for the target DB
  -c, --collection <string>  the name of the collection to be sync'ed
  -d, --timestamp <string>   the transtamp from which to start syncing
  -h, --help                 display help for command
```

Example
```$ node main.js --source $SOURCE_KEY --target $TARGET_KEY -c Customer -d 1698884566000```


### Best Practices
- To avoid gaps in synchronization, you should usea start timestamp less than the timestamp of the last synced wriet on the target collection.
- To reduce the overall time to sync an entire database, run one instance of this tool for each collection, in parallel


## Process for Migration

1. Check that all prerequisites listed earlier in this README have been met
2. Create a database(B) from the in the target RG from the latest available snapshot of the database A.
2. Generate admin key for database B
3. Run the script in `main.js`, passing the required authentication keys, the name of the collection to be sync'ed, and the transaction time from which write history is to be applied to the target database
 - Monitor the time it takes to perform this update, as this is the theoretical minimum downtime that can be achieved during cutover. As a Best practice run this update on a regular basis for two reasons:
  - Get an typical baseline of the time needed to sync the database with the latest writes since the tool was last run
  - Keeping the target collection frequently sync'ed with the source database gives you flexibility to abort an application cutover
4. Schedule the application cutover
 - It's Recommended that this be scheduled for a window when the downtime least impacts your application workload
5. Cutover your Application 

Application cutover is the action of transitioning your application from using the source database and reconnecting it to the target database. The strategy for you cutover involves changing the access keys your application uses to connect to the Fauna. Application cutover occurs when you have replaced the keys which connect your applicaiton to the source database with keys that connect to the target database. The general procudure for application cutover is: 
1. Disable writes from the application
2. Confirm no new writes are occurring
3. Run a final execution of this tool to sync the last of the writes 
4. Update access keys in the application with keys pointing to the target database


## Limitations

- Documents with over 100,000 events will only have the first set of 100,000 events copied over.
- Any new schema documents (collections, indexes) created after the snapshot was copied will not be migrated. Usage of this tool is not recommended while schema documents are modified.
- Creates, updates, and deletes applied after the snapshot was taken will be copied in order by this script but using the current time. In other words, the `ts` field's value is not preserved.
- Usage of history manipulation is incompatible with this script. Because this script only looks at events in time order going forward, it will miss events manipulated in the past.
